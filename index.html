<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Face Recognition Classifier</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    #result {
      font-weight: bold;
      font-size: 2rem;
      text-align: center;
      min-height: 3rem;
    }
    #confidence {
      font-size: 1.2rem;
      text-align: center;
      color: #666;
    }
    #model-info {
      font-size: 1rem;
      background: #f8f9fa;
      padding: 10px;
      border-radius: 5px;
      margin: 10px 0;
    }
    #camera-container {
      position: relative;
      margin-bottom: 20px;
    }
    #camera-feed {
      width: 100%;
      max-width: 500px;
      border-radius: 5px;
    }
    #processing-canvas {
      display: none;
    }
    .class-progress {
      height: 25px;
      margin-bottom: 5px;
    }
    .progress-label {
      position: absolute;
      left: 10px;
      color: white;
      text-shadow: 1px 1px 1px #000;
    }
    .class-container {
      margin-top: 20px;
    }
    #class-progress-container {
      max-width: 500px;
      margin: 0 auto;
    }
    #camera-select {
      margin-bottom: 10px;
    }
    .face-box {
      position: absolute;
      border: 2px solid #0f0;
      background: rgba(0, 255, 0, 0.1);
      display: none;
    }
  </style>
</head>
<body>
  <main>
    <div class="px-4 py-2 my-2 text-center border-bottom">
      <h1 class="display-5 fw-bold">Face Recognition Classifier</h1>
      <div class="col-lg-6 mx-auto">
        <p class="lead mb-0">Real-time face detection and classification with TensorFlow.js</p>
      </div>
    </div>

    <div class="container mt-4">
      <div class="row justify-content-center">
        <div class="col-12 col-md-8 text-center">
          <!-- Camera Feed -->
          <div id="camera-container">
            <video id="camera-feed" autoplay playsinline></video>
            <div id="face-box" class="face-box"></div>
            <canvas id="processing-canvas" width="224" height="224"></canvas>
          </div>
          
          <!-- Camera Selection -->
          <select id="camera-select" class="form-select d-none">
            <option value="">Select camera...</option>
          </select>
          
          <!-- Model Info -->
          <div id="model-info" class="text-start">
            <div>Models: <span id="model-name">Loading face detection and classification models...</span></div>
            <div>Status: <span id="model-status">Loading...</span></div>
          </div>
          
          <!-- Prediction Results -->
          <div id="result" class="my-3">Loading models...</div>
          <div id="confidence"></div>
          
          <!-- Class Probabilities -->
          <div id="class-progress-container" class="class-container">
            <h4>Class Probabilities</h4>
            <div id="class-progress-bars"></div>
          </div>

          <!-- Controls -->
          <div class="mt-3">
            <button id="switch-camera" class="btn btn-secondary">Switch Camera</button>
          </div>
        </div>
      </div>
    </div>
  </main>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.5.0/dist/tf.min.js"></script>
  
  <script>
    // App State
    const state = {
      faceDetectionModel: null,
      classificationModel: null,
      stream: null,
      facingMode: 'environment',
      devices: [],
      currentDeviceId: null,
      classNames: ['Angel', 'Celia', 'Eddel', 'Federico', 'Feliciano', 
                  'Fernando', 'Jennifer', 'Jesus', 'Kevin', 'Manuel', 'Sebastian'],
      isModelLoading: true,
      faceDetected: false
    };

    // DOM Elements
    const elements = {
      video: document.getElementById('camera-feed'),
      processingCanvas: document.getElementById('processing-canvas'),
      result: document.getElementById('result'),
      confidence: document.getElementById('confidence'),
      modelName: document.getElementById('model-name'),
      modelStatus: document.getElementById('model-status'),
      cameraSelect: document.getElementById('camera-select'),
      switchCamera: document.getElementById('switch-camera'),
      classProgressBars: document.getElementById('class-progress-bars'),
      faceBox: document.getElementById('face-box')
    };

    // Initialize
    document.addEventListener('DOMContentLoaded', async () => {
      if (!checkWebGL() || !checkTFJS()) {
        elements.result.textContent = "Your browser doesn't support all required features";
        return;
      }

      try {
        await tf.ready();
        console.log('TensorFlow.js is ready');
        
        // Load models
        await loadModels();
        
        // Setup camera
        await getCameraDevices();
        if (state.devices.length > 0) {
          state.currentDeviceId = state.devices[0].deviceId;
          populateCameraSelect();
        }
        await setupCamera();
        
        setupEventListeners();
        detectAndClassify();

      } catch (error) {
        console.error("Initialization error:", error);
        elements.result.textContent = `Error: ${error.message}`;
        elements.modelStatus.textContent = `Error: ${error.message}`;
      }
    });

    function checkWebGL() {
      const gl = document.createElement('canvas').getContext('webgl');
      if (!gl) {
        alert('WebGL is not available in your browser. The app may not work properly.');
        return false;
      }
      return true;
    }

    function checkTFJS() {
      if (!tf || !tf.browser || !tf.loadGraphModel) {
        alert('TensorFlow.js is not loaded correctly. Please refresh the page.');
        return false;
      }
      return true;
    }

    async function loadModels() {
      const faceDetectionModelPath = 'https://tfhub.dev/tensorflow/tfjs-model/blazeface/1/default/1';
      const classificationModelPath = 'model/model.json';
      
      try {
        // Load face detection model
        state.faceDetectionModel = await tf.loadGraphModel(faceDetectionModelPath, { fromTFHub: true });
        console.log('Face detection model loaded successfully');
        
        // Load classification model
        const response = await fetch(classificationModelPath);
        if (!response.ok) {
          throw new Error(`Classification model not found at ${classificationModelPath}`);
        }
        state.classificationModel = await tf.loadGraphModel(classificationModelPath);
        console.log('Classification model loaded successfully');
        
        state.isModelLoading = false;
        elements.modelName.textContent = "BlazeFace (detection) + Custom (classification)";
        elements.modelStatus.textContent = "All models loaded successfully";
        
        // Initialize class progress bars
        initClassProgressBars();
        
      } catch (error) {
        console.error('Error loading models:', error);
        elements.modelStatus.textContent = `Error loading models`;
        throw error;
      }
    }

    function initClassProgressBars() {
      elements.classProgressBars.innerHTML = '';
      state.classNames.forEach((className, i) => {
        const div = document.createElement('div');
        div.className = 'progress class-progress';
        div.innerHTML = `
          <div id="progress-${i}" class="progress-bar" role="progressbar" style="width: 0%" 
               aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">
            <span class="progress-label">${className}</span>
          </div>
        `;
        elements.classProgressBars.appendChild(div);
      });
    }

    async function setupCamera() {
      if (state.stream) {
        state.stream.getTracks().forEach(track => track.stop());
      }

      const constraints = {
        video: {
          width: { ideal: 500 },
          height: { ideal: 500 },
          facingMode: isMobile() ? state.facingMode : 'user',
          deviceId: state.currentDeviceId ? { exact: state.currentDeviceId } : undefined
        }
      };

      try {
        state.stream = await navigator.mediaDevices.getUserMedia(constraints);
        elements.video.srcObject = state.stream;
        await elements.video.play();
      } catch (err) {
        console.error("Camera error:", err);
        elements.result.textContent = "Error accessing camera";
      }
    }

    function isMobile() {
      return /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
    }

    function prepareFaceDetectionInput() {
      return tf.tidy(() => {
        // Draw current video frame to processing canvas
        const ctx = elements.processingCanvas.getContext('2d');
        ctx.drawImage(elements.video, 0, 0, 224, 224);
        
        // Convert to tensor and preprocess for BlazeFace
        return tf.browser.fromPixels(elements.processingCanvas)
          .toFloat()
          .div(255)
          .expandDims(0);
      });
    }

    function prepareClassificationInput(faceBox) {
      return tf.tidy(() => {
        const ctx = elements.processingCanvas.getContext('2d');
        
        // Calculate crop coordinates
        const videoWidth = elements.video.videoWidth;
        const videoHeight = elements.video.videoHeight;
        
        // Get face bounding box coordinates (adjusting for video dimensions)
        const startX = faceBox[0] * videoWidth;
        const startY = faceBox[1] * videoHeight;
        const width = (faceBox[2] - faceBox[0]) * videoWidth;
        const height = (faceBox[3] - faceBox[1]) * videoHeight;
        
        // Draw the face region to the processing canvas
        ctx.clearRect(0, 0, 224, 224);
        ctx.drawImage(
          elements.video, 
          startX, startY, width, height,  // source rectangle (face region)
          0, 0, 224, 224                 // destination rectangle (full canvas)
        );
        
        // Convert to tensor and preprocess for classification model
        return tf.browser.fromPixels(elements.processingCanvas)
          .toFloat()
          .div(255.0)
          .expandDims(0);
      });
    }

    async function detectFaces() {
      if (state.isModelLoading || !state.faceDetectionModel) {
        return null;
      }

      try {
        const inputTensor = prepareFaceDetectionInput();
        const predictions = await state.faceDetectionModel.executeAsync(inputTensor);
        inputTensor.dispose();
        
        // Get the face bounding boxes
        const boxes = await predictions[0].array();
        const scores = await predictions[1].array();
        
        // Clean up
        tf.dispose(predictions);
        
        // Find the face with highest score
        let maxScore = 0;
        let bestFace = null;
        
        for (let i = 0; i < scores[0].length; i++) {
          if (scores[0][i] > maxScore) {
            maxScore = scores[0][i];
            bestFace = boxes[0][i];
          }
        }
        
        // Only consider it a face if confidence is above 0.7
        if (maxScore > 0.7) {
          return bestFace;
        }
        
        return null;
        
      } catch (error) {
        console.error("Face detection error:", error);
        return null;
      }
    }

    function updateFaceBox(faceBox) {
      if (!faceBox) {
        elements.faceBox.style.display = 'none';
        state.faceDetected = false;
        return;
      }
      
      const videoWidth = elements.video.videoWidth;
      const videoHeight = elements.video.videoHeight;
      
      // Calculate box dimensions
      const startX = faceBox[0] * videoWidth;
      const startY = faceBox[1] * videoHeight;
      const width = (faceBox[2] - faceBox[0]) * videoWidth;
      const height = (faceBox[3] - faceBox[1]) * videoHeight;
      
      // Position the face box
      elements.faceBox.style.display = 'block';
      elements.faceBox.style.left = `${startX}px`;
      elements.faceBox.style.top = `${startY}px`;
      elements.faceBox.style.width = `${width}px`;
      elements.faceBox.style.height = `${height}px`;
      
      state.faceDetected = true;
    }

    async function classifyFace(faceBox) {
      if (state.isModelLoading || !state.classificationModel || !faceBox) {
        return null;
      }

      try {
        const inputTensor = prepareClassificationInput(faceBox);
        const predictions = await state.classificationModel.predict(inputTensor);
        inputTensor.dispose();
        
        return predictions;
      } catch (error) {
        console.error("Classification error:", error);
        return null;
      }
    }

    function updateUI(predictions) {
      if (!state.faceDetected) {
        elements.result.textContent = "Apunta la camara a una cara";
        elements.confidence.textContent = "";
        
        // Reset all progress bars
        state.classNames.forEach((_, i) => {
          const progressBar = document.getElementById(`progress-${i}`);
          progressBar.style.width = "0%";
          progressBar.setAttribute('aria-valuenow', 0);
          progressBar.querySelector('.progress-label').textContent = state.classNames[i];
        });
        
        return;
      }
      
      // Get top prediction
      const topPredIndex = predictions.argMax(1).dataSync()[0];
      const topPredConfidence = predictions.max(1).dataSync()[0] * 100;
      
      // Update result display
      elements.result.textContent = state.classNames[topPredIndex];
      elements.confidence.textContent = `Confidence: ${topPredConfidence.toFixed(1)}%`;
      
      // Update all progress bars
      const predArray = predictions.dataSync();
      state.classNames.forEach((_, i) => {
        const confidence = predArray[i] * 100;
        const progressBar = document.getElementById(`progress-${i}`);
        progressBar.style.width = `${confidence}%`;
        progressBar.setAttribute('aria-valuenow', confidence);
        progressBar.querySelector('.progress-label').textContent = 
          `${state.classNames[i]}: ${confidence.toFixed(1)}%`;
      });
    }

    async function detectAndClassify() {
      if (state.isModelLoading || !state.faceDetectionModel) {
        setTimeout(detectAndClassify, 100);
        return;
      }

      try {
        // First detect faces
        const faceBox = await detectFaces();
        updateFaceBox(faceBox);
        
        if (faceBox) {
          // If face detected, classify it
          const predictions = await classifyFace(faceBox);
          if (predictions) {
            updateUI(predictions);
            tf.dispose(predictions);
          }
        } else {
          // No face detected
          updateUI(null);
        }
      } catch (error) {
        console.error("Detection/classification error:", error);
      } finally {
        requestAnimationFrame(detectAndClassify);
      }
    }

    async function getCameraDevices() {
      if (!navigator.mediaDevices.enumerateDevices) return;
      
      try {
        const devices = await navigator.mediaDevices.enumerateDevices();
        state.devices = devices.filter(device => device.kind === 'videoinput');
      } catch (err) {
        console.error("Error enumerating devices:", err);
      }
    }

    function populateCameraSelect() {
      elements.cameraSelect.innerHTML = '<option value="">Select camera...</option>';
      state.devices.forEach(device => {
        const option = document.createElement('option');
        option.value = device.deviceId;
        option.text = device.label || `Camera ${elements.cameraSelect.length}`;
        elements.cameraSelect.appendChild(option);
      });
    }

    function setupEventListeners() {
      elements.switchCamera.addEventListener('click', switchCamera);
      elements.cameraSelect.addEventListener('change', async (e) => {
        state.currentDeviceId = e.target.value;
        await setupCamera();
      });
    }

    async function switchCamera() {
      if (isMobile()) {
        state.facingMode = state.facingMode === 'user' ? 'environment' : 'user';
      } else {
        elements.cameraSelect.classList.toggle('d-none');
        return;
      }
      await setupCamera();
    }
  </script>
</body>
</html>